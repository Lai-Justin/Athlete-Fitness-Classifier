# Athlete Fitness Classifier

## Introduction
For machine problem one, I created a binary classifier that classifies anonymous individuals as fit or unfit based on 11 different health criteria such as weight, height, and gender. The project employs the use of the Naïve Bayes Algorithm, a core concept in statistics and the basis of many probabilistic machine learning models today. The following report outlines the steps taken to create this classifier, including the architecture of the code, preprocessing steps, model building, testing, results, and the challenges and weaknesses faced while completing this project.

## Architecture
To write the program for this binary classifier, it was decided that Python would be the most effective programming language to write the program for its ease of use in writing the scripts that represent the Naïve Bayes Algorithm as well as allowing me to take advantage of the multitude of dedicated data science and machine learning libraries available to better contain and process the data. At its core, the program consists of two main functions, the training function and the testing function.

As its name suggests, the training function is the core part of the program as it trains the AI model used to predict and classify each individual in the testing data set on their fitness. This was done using a Naïve Bayes Classifier, a popular probabilistic machine learning model based on the Bayes’ Theorem. Based on Bayes’ Theorem, one would be able to calculate the probability of an event A occurring given the probability of another event B occurring. This rule can also be extended to include a multitude of other events C, D, E, etc. assuming that all of the conditional events are independent of each other. In reality, it is unlikely that all events that are conditional for another event to occur will be independent, and as such, we will be assuming independence, hence the name Naïve. For this problem in particular, we will be assuming that all 11 given health criteria given in the training data are all independent of each other, despite that not being the case in reality, in order to calculate the probability of the specific individual being classified as fit or not. Using the Naïve Bayes Algorithm, the conditional probability of an individual being classified as fit given all 11 of their health criteria will be the product of all individual conditional probabilities of the health criteria given their fitness class multiplied by the probability of the fitness classification among the entire data.

## Preprocessing
To start, the entire program begins with the training function. There, the program opens and reads through each line of “training.txt”, of which each row contains the 11 values representing the 11 different health criteria of each individual. While reading through each line of the training set, the strings of text are stripped and cleaned of any unnecessary clutter such as punctuation before being stored in a two-dimensional array called “trainingData”. As as well as this, as this binary classifier is being trained on supervised training data, the end of each row also contains the correct label of whether or not the individual is considered fit. This information is stored in a hash set named “results” which contains the counts of all “successes” and “failures”, which will be used to create the conditional probabilities.

## Model Building
As stated above, this classifier uses the Naïve Bayes Algorithm which requires finding the product of all of the independent conditional probabilities of all 11 given health criteria. To do this, we must calculate all 11 different conditional probabilities. This was done by creating a helper two-dimensional array called “toGetPercentiles” with 11 individual arrays containing all health data representing just one criterion. Next, the program uses the NumPy percentiles function to calculate the 25th, 50th, 75, and 100th percentile of each individual healthy criteria and stores this all within a “percentiles” two-dimensional array. From there, we then create a three-dimensional array called “trainingData3D”. Here, the first dimension is simply two entries, with the first entry representing a two-dimensional array of all unfit individuals, and the second entry representing all fit individuals. Going into the next dimension, there are 11 different arrays corresponding to the 11 different features used in the classification, such as weight, height, and heart rate. And lastly, the next dimension is of length four, with each entry representing the individual and where they stand along the four percentile ranges from 0%-25%, 25%-50%, 50%-75%, and 75%-100% among each health criteria. By looping through the entire training data set, we can then fill “trainingData3D” with the counts of all individuals and all health criteria in every percentile. Next, we can then iterate through “trainingData3D” and divide each count within the three-dimensional array by the total count of failures and successes respectively in order to get all of the conditional probabilities for each of the 11 different health criteria. And with “trainingData3D” completed, we now have a complete model from which we can use the Naïve Bayes Algorithm to predict the probability of fitness for an individual person.

## Testing
For the last part of the project, I ran through the program with a testing dataset that contains 2000 individuals and their health data but this time, without the fitness class in it. Within the testing function of the program is a similar block of code to the data preprocessing section in the training function, which reads the testing data from command line arguments, cleaning each line of text from its punctuation, and storing all of the information in a two-dimensional array named “testingData”. With the testing data processed and stored in an array, we can then begin using our “trainingData3D” to calculate the conditional probability of fitness or unfitness based on the Naïve Bayes Algorithm. For our purposes of predicting a binary classification, we will be calculating simultaneously the probability of fitness and unfitness, which we will then compare at the end to return the final prediction. Since the algorithm requires multiplying the product of conditional probabilities by the probability of the predicted class, the program will use this as to represent the calculations. We can then iterate through the testing data line by line, finding where each health statistic lies along its respective percentiles, and then multiplying the corresponding conditional probability to the variables representing fit and unfit probabilities. Finally, we then compare the two conditional probabilities, and whichever score is a higher probability will be the class that the program predicts. With the testing portion of the program completed results in an accuracy on the testing dataset of 78%.

## Results
With the athlete fitness binary classifier completed, the program was then tested on a private, undisclosed testing dataset. Despite certain assumptions being made in order to use the Naïve Bayes algorithm, such as having to assume that all features being tested are independent of each other, the algorithm was still able to achieve a high accuracy of 77.45% on the private testing dataset. As well as this, the simplicity of the Naïve Bayes classifier allows us to train and test on over 10,000 samples with a total running time of 0.5 seconds, showcasing the efficiency and low processing demands of this algorithm.
